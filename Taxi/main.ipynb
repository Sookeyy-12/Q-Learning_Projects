{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Taxi-v3 using:\n",
    "- render_mode = \"rgb_array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the environment Looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "Observation Space: Discrete(500)\n",
      "Sample Observation Space: 370\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Sample Observation Space: {env.observation_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "- Destination on the map are represented with the first letter of the color.\n",
    "    - Passenger Locations: \n",
    "        - 0: Red\n",
    "        - 1: Green\n",
    "        - 2: Yellow\n",
    "        - 3: Blue\n",
    "        - 4: In taxi\n",
    "    - Destinations:\n",
    "        - 0: Red\n",
    "        - 1: Green\n",
    "        - 2: Yellow\n",
    "        - 3: Blue\n",
    "- An observation is returned as an int() that encodes the corresponding state, calculated by ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _____ACTION SPACE_____ \n",
      "Action Space: 6\n",
      "Sample Action Space: 1\n"
     ]
    }
   ],
   "source": [
    "print(\" _____ACTION SPACE_____ \")\n",
    "print(f\"Action Space: {env.action_space.n}\")\n",
    "print(f\"Sample Action Space: {env.action_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action shape is (1,) in the range {0, 5} indicating which direction to move the taxi or to pickup/drop off passengers.\n",
    "- 0: Move south (down)\n",
    "- 1: Move north (up)\n",
    "- 2: Move ea\n",
    "- 3: Move west (left)\n",
    "- 4: Pickup passenger\n",
    "- 5: Drop off passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward Function:\n",
    "- -1 per step unless other reward is triggered.\n",
    "- +20 delivering passenger.\n",
    "- -10 executing “pickup” and “drop-off” actions illegally.\n",
    "<br>\n",
    "\n",
    "An action that results a noop, like moving into a wall, will incur the time step penalty. Noops can be avoided by sampling the action_mask returned in info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Initialize Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 500 possible states (rows)\n",
      "There are 6 possible actions (columns)\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(f\"There are {state_space} possible states (rows)\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(f\"There are {action_space} possible actions (columns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Q-Table of size (state_space, action_space) and initializing values to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros((state_space, action_space))\n",
    "    return Qtable\n",
    "\n",
    "Qtable_Taxi = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    # Exploitation, take the action with the highest state-action value.\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # Randomly generate a number from 0 to 1:\n",
    "    random_num = random.uniform(0,1)\n",
    "    # if random_num greater than epsilon --> exploitation\n",
    "    if random_num > epsilon:\n",
    "        # Take action with the highest value given a state\n",
    "        action = greedy_policy(Qtable, state)\n",
    "    # else --> exploration\n",
    "    else :\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "n_training_episodes = 50000     # total training episodes\n",
    "learning_rate = 0.7             # learning rate\n",
    "\n",
    "# Environment Parameters\n",
    "env_id = \"Taxi-v3\"              # name of the environment\n",
    "max_steps = 99                  # Max steps per episode\n",
    "gamma = 0.95                    # Discounting Rate\n",
    "\n",
    "# Exploration Parameters\n",
    "max_epsilon = 1.0               # Exploration Probability at start\n",
    "min_epsilon = 0.05              # Minimum Exploration Probabilty\n",
    "decay_rate = 0.005              # exponential decay rate for exploration probabilty\n",
    "\n",
    "# Recording Parameters\n",
    "save_video = True               # Save the video or not\n",
    "save_frequency = 10000          # no of episodes after which a video is saved\n",
    "fps = 3                        # fps of the saved video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, max_epsilon, min_epsilon, decay_rate, save_video, save_frequency, max_steps, Qtable, fps, env):\n",
    "\n",
    "    images = []     # to store images to convert into videos\n",
    "\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reduce epsilon because we need less and less exploration as we proceed\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "\n",
    "        # Reset the environment\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Create the first frame and append it to the images if save frequency has passed or its last episode\n",
    "        if save_video and (episode%save_frequency==0 or episode==n_training_episodes-1):\n",
    "            img = env.render()\n",
    "            images.append(img)\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            # Take action At and observe state s' and reward r\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Update Q(s,a) := Q(s,a) + lr * [R(s,a) + gamma * max(Q(s',a')) - Q(s,a)]\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "            # Create a frame and append it to the images if save frequency has passed or its last episode\n",
    "            if save_video and (episode%save_frequency==0 or episode==n_training_episodes-1):\n",
    "                img = env.render()\n",
    "                images.append(img)\n",
    "\n",
    "            # If terminated or truncated, finish the episode\n",
    "            if truncated or terminated:\n",
    "                break\n",
    "\n",
    "            # Our next state is the new state\n",
    "            state = new_state\n",
    "\n",
    "        # if save frequency has passed or its last episode convert the images[] to a video\n",
    "        if save_video and (episode%save_frequency==0 or episode==n_training_episodes-1):\n",
    "            plt.close()     # Close the plot after saving each frame\n",
    "            vid_name = f\"./Videos/training-{episode}.mp4\"\n",
    "            imageio.mimsave(vid_name, images, fps=fps, macro_block_size=1)      # Save the images as a video file\n",
    "            images = []     # Reset the images array for new video\n",
    "\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:24<00:00, 2056.30it/s]\n"
     ]
    }
   ],
   "source": [
    "Qtable_Taxi = train(n_training_episodes, max_epsilon, min_epsilon, decay_rate, save_video, save_frequency, max_steps, Qtable_Taxi, fps, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  2.75200369,   3.94947757,   2.75200369,   3.94947757,\n",
       "          5.20997639,  -5.05052243],\n",
       "       [  7.93349184,   9.40367562,   7.93349184,   9.40367562,\n",
       "         10.9512375 ,   0.40367562],\n",
       "       ...,\n",
       "       [  6.81233243,  12.58025   ,  -2.65639999,   5.78016944,\n",
       "        -11.37820285,  -2.04870545],\n",
       "       [  4.48501837,  -3.06295667,  -3.03177213,   6.53681725,\n",
       "         -5.38301653, -10.33515   ],\n",
       "       [ 15.64639998,  -1.57815   ,  11.06      ,  18.        ,\n",
       "         -7.        ,   5.3837    ]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable_Taxi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
