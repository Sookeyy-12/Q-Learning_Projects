{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have two sizes of environment:\n",
    "- `map_name=\"4x4\"`: a 4x4 grid version\n",
    "- `map_name=\"8x8\"`: a 8x8 grid version\n",
    "\n",
    "The environment has two modes:\n",
    "- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
    "- `is_slippery=True`: The agent may not always move in the intended direction due to the slippery nature of the frozen lake (stochastic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the FrozenLake-v1 environment using:\n",
    "- 4x4 map.\n",
    "- non-slippery version.\n",
    "- render_mode = \"rgb_array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=False, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create your own custom grid like this:<br>\n",
    "`desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]`<br>\n",
    "`gym.make('FrozenLake-v1', desc=desc, is_slippery=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the environment looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "Observation Space: Discrete(64)\n",
      "Sample Observation: 53\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Sample Observation: {env.observation_space.sample()}\")  # get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see with Observation Space Shape Discrete(16) that the observation is an integer representing the agent’s current position as current_row * nrows + current_col (where both the row and col start at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _____ACTION SPACE_____ \n",
      "Action Space Shape: 4\n",
      "Action Space Sample: 3\n"
     ]
    }
   ],
   "source": [
    "print(\" _____ACTION SPACE_____ \")\n",
    "print(f\"Action Space Shape: {env.action_space.n}\")\n",
    "print(f\"Action Space Sample: {env.action_space.sample()}\")  # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space (the set of possible actions the agent can take) is discrete with 4 actions available:\n",
    "- 0: GO LEFT\n",
    "- 1: GO DOWN\n",
    "- 2: GO RIGHT\n",
    "- 3: GO UP\n",
    "\n",
    "Reward function:\n",
    "- Reach goal: +1\n",
    "- Reach hole: 0\n",
    "- Reach frozen tile: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Initialize Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 64 possible states (rows)\n",
      "There are 4 possible actions (columns)\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(f\"There are {state_space} possible states (rows)\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(f\"There are {action_space} possible actions (columns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Q-Table of size (state_space, action_space) and initialize each value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros((state_space, action_space))\n",
    "    return Qtable\n",
    "\n",
    "Qtable_FrozenLake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    # Exploitation, take the action with the highest state-action value.\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # Randomly generate a number from 0 to 1\n",
    "    random_num = random.uniform(0, 1)\n",
    "    # if random_num greater than epsilon --> exploitation\n",
    "    if random_num > epsilon:\n",
    "        # Take the action with the highest value given a state\n",
    "        action = greedy_policy(Qtable, state)\n",
    "    # else --> exploration\n",
    "    else: \n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "n_training_episodes = 400000     # total training episodes\n",
    "learning_rate = 0.8             # learning rate\n",
    "\n",
    "# Environment Parameters\n",
    "env_id = \"FrozenLake-v1\"        # name of the environment\n",
    "max_steps = 100                 # Max steps per episode\n",
    "gamma = 0.90                    # Discounting Rate\n",
    "\n",
    "# Exploration Parameters\n",
    "max_epsilon = 1.0               # Exploration Probability at start\n",
    "min_epsilon = 0.05              # Minimum Exploration Probabilty\n",
    "decay_rate = 0.00001             # exponential decay rate for exploration probabilty\n",
    "\n",
    "save_video=True\n",
    "save_frequency = 100000\n",
    "fps = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training loop method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable, save_video, save_frequency, fps):\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reduce epsilon because we need less and less exploration as we proceed\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
    "\n",
    "        # Reset the environment\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        if save_video and (episode%save_frequency==0 or episode == n_training_episodes-1):\n",
    "            img = env.render()\n",
    "            images.append(img)\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            # Take action At and observe state s' and reward r\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Update Q(s,a) := Q(s,a) + lr * [R(s,a) + gamma * max(Q(s',a')) - Q(s,a)]\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]) \n",
    "\n",
    "            if save_video and (episode%save_frequency==0 or episode == n_training_episodes-1):\n",
    "                img = env.render()\n",
    "                images.append(img)\n",
    "\n",
    "            # If terminated or truncated, finish the episode\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            # Our next state is the new state\n",
    "            state = new_state\n",
    "\n",
    "        if save_video and (episode%save_frequency==0 or episode == n_training_episodes-1):\n",
    "            plt.close()  # Close the plot after saving each frame\n",
    "            vid_name = './Videos/training' + f'{episode}' + '.mp4'\n",
    "            imageio.mimsave(vid_name, images, fps=fps)  # Save the images as a video file\n",
    "            images = []\n",
    "\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Q-Learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [02:45<00:00, 2419.19it/s]\n"
     ]
    }
   ],
   "source": [
    "Qtable_FrozenLake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_FrozenLake, save_video, save_frequency, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22876792, 0.25418658, 0.25418658, 0.22876792],\n",
       "       [0.22876792, 0.28242954, 0.28242954, 0.25418658],\n",
       "       [0.25418658, 0.3138106 , 0.3138106 , 0.28242954],\n",
       "       [0.28242954, 0.34867844, 0.34867844, 0.3138106 ],\n",
       "       [0.3138106 , 0.38742049, 0.38742049, 0.34867844],\n",
       "       [0.34867844, 0.43046721, 0.43046721, 0.38742049],\n",
       "       [0.38742049, 0.4782969 , 0.4782969 , 0.43046721],\n",
       "       [0.43046721, 0.531441  , 0.4782969 , 0.4782969 ],\n",
       "       [0.25418658, 0.28242954, 0.28242954, 0.22876792],\n",
       "       [0.25418658, 0.3138106 , 0.3138106 , 0.25418658],\n",
       "       [0.28242954, 0.34867844, 0.34867844, 0.28242954],\n",
       "       [0.3138106 , 0.        , 0.38742049, 0.3138106 ],\n",
       "       [0.34867844, 0.43046721, 0.43046721, 0.34867844],\n",
       "       [0.38742049, 0.4782969 , 0.4782969 , 0.38742049],\n",
       "       [0.43046721, 0.531441  , 0.531441  , 0.43046721],\n",
       "       [0.4782969 , 0.59049   , 0.531441  , 0.4782969 ],\n",
       "       [0.28242954, 0.3138106 , 0.3138106 , 0.25418658],\n",
       "       [0.28242954, 0.34867844, 0.34867844, 0.28242954],\n",
       "       [0.3138106 , 0.38742049, 0.        , 0.3138106 ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.4782969 , 0.4782969 , 0.38742049],\n",
       "       [0.43046721, 0.        , 0.531441  , 0.43046721],\n",
       "       [0.4782969 , 0.59049   , 0.59049   , 0.4782969 ],\n",
       "       [0.531441  , 0.6561    , 0.59049   , 0.531441  ],\n",
       "       [0.3138106 , 0.28242954, 0.34867844, 0.28242954],\n",
       "       [0.3138106 , 0.3138106 , 0.38742049, 0.3138106 ],\n",
       "       [0.34867844, 0.34867844, 0.43046721, 0.34867844],\n",
       "       [0.38742049, 0.        , 0.4782969 , 0.        ],\n",
       "       [0.43046721, 0.531441  , 0.        , 0.43046721],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.6561    , 0.6561    , 0.531441  ],\n",
       "       [0.59049   , 0.729     , 0.6561    , 0.59049   ],\n",
       "       [0.28242954, 0.25418658, 0.3138106 , 0.3138106 ],\n",
       "       [0.28242954, 0.        , 0.34867844, 0.34867844],\n",
       "       [0.3138106 , 0.        , 0.        , 0.38742049],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.59049   , 0.59049   , 0.4782969 ],\n",
       "       [0.531441  , 0.6561    , 0.6561    , 0.        ],\n",
       "       [0.59049   , 0.        , 0.729     , 0.59049   ],\n",
       "       [0.6561    , 0.81      , 0.729     , 0.6561    ],\n",
       "       [0.25418658, 0.28242954, 0.        , 0.28242954],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.4782969 , 0.59049   , 0.        ],\n",
       "       [0.531441  , 0.        , 0.6561    , 0.531441  ],\n",
       "       [0.59049   , 0.729     , 0.        , 0.59049   ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9       , 0.81      , 0.729     ],\n",
       "       [0.28242954, 0.3138106 , 0.        , 0.25418658],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.38742049, 0.4782969 , 0.        ],\n",
       "       [0.43046721, 0.        , 0.        , 0.531441  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.81      , 0.        , 0.6561    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.9       , 0.81      ],\n",
       "       [0.3138106 , 0.3138106 , 0.34867844, 0.28242954],\n",
       "       [0.3138106 , 0.34867844, 0.38742049, 0.        ],\n",
       "       [0.34867844, 0.38742049, 0.        , 0.43046721],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.729     , 0.81      , 0.        ],\n",
       "       [0.729     , 0.81      , 0.9       , 0.729     ],\n",
       "       [0.81      , 0.9       , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable_FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def record_video(env, Qtable, out_directory, fps=1):\n",
    "#   \"\"\"\n",
    "#   Generate a replay video of the agent\n",
    "#   :param env\n",
    "#   :param Qtable: Qtable of our agent\n",
    "#   :param out_directory\n",
    "#   :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "#   \"\"\"\n",
    "#   images = []  \n",
    "#   terminated = False\n",
    "#   truncated = False\n",
    "#   state, info = env.reset(seed=random.randint(0,500))\n",
    "#   img = env.render()\n",
    "#   images.append(img)\n",
    "#   while not terminated or truncated:\n",
    "#     # Take the action (index) that have the maximum expected future reward given that state\n",
    "#     action = np.argmax(Qtable[state][:])\n",
    "#     state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "#     img = env.render()\n",
    "#     images.append(img)\n",
    "#   imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_directory=f\"./Videos/final.mp4\"\n",
    "# record_video(env, Qtable_FrozenLake, out_directory, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
